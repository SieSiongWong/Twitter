---
title: "Final Project"
author: "Sie Siong Wong - Joe Rovalino - Anil Akyildirim"
date: "11/8/2019"
output: html_document
---

Siong Version:
---------------------------------------------------------------------------------------------------------------------------------

```{r, eval=TRUE}

installed.packages('base64enc')

```

```{r, eval=TRUE, warning=FALSE, message=FALSE}

# Load Requried Packages

library(tm)
library(dplyr)
library(tidyverse)
library(tidyr)
library(tidytext)
library("SnowballC")
library("tm")
library("twitteR")
library("rtweet")
library("syuzhet")
library("httr")
library("lda")
library("rJava")
library("coreNLP")

```

## Example of getting tweets from Twitter through API.

```{r, eval=TRUE}

app_name <- "JAS"
consumer_key <- 'sPwbbZCtf8nfSMxhYTzqI8WHJ'
consumer_secret <- 'KfcOxgElcQ70fi3QNy8LkuDAN18dunXT147MoA8aBOLzpr3Vd3'
access_token <- '600477513-rdd3Fcywq1sfnh5S60egRQxXh0TlDqfrLzyZo4Vk'
access_secret <- 'SdDFCJUOoqAwt671VXeLaD781TdUYdeBSW2gyQMG4P5Zh'

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tweets <- userTimeline("realDonaldTrump", n=1)
tweets

```

## Loading of Data

```{r, eval=TRUE}

tweets_raw <- read.csv("https://raw.githubusercontent.com/SieSiongWong/Twitter/dev/trumptweets.csv")

stocks_raw <- read.csv("https://raw.githubusercontent.com/SieSiongWong/Twitter/dev/sandp.csv")

```

## Pre-Processing

### Stock Data Cleaning

```{r, eval}

# Update Date column into date format.
stocks_raw$Date <- as.Date(stocks_raw$Date)

# Select data from date 2018 January 01 and do percentage change between closing and opening price.
stocks.df <- stocks_raw %>% 
  filter(between(Date, as.Date("2018-01-01"),as.Date("2019-11-20"))) %>%
  mutate(Pct_Change=(Close-Open)/Open*100)

```


### Tweets Data Cleaning 

```{r, eval=TRUE}

# Extract columns from trumptweets csv that are useful for analysis.
tweets_slc <- tweets_raw %>% select(source, text, created_at) 

# Remove source other than iphone.
tweets_slc <- tweets_slc %>% filter(source=="Twitter for iPhone")

# Drop source column.
tweets_slc <- tweets_slc %>% select(text, created_at)

# Separate column "created_at" into "date" and "hour".
tweets_slc <- separate(data = tweets_slc, col = created_at, into  = c('date', 'hour'), sep = ' ') %>% select(text, date, hour)

# Remove minutes in hour column.
tweets_slc$hour <- gsub("\\:+\\w*","", tweets_slc$hour)

# Remove meaningless characters and symbols.
tweets_slc$text <- gsub("&amp","", tweets_slc$text)
tweets_slc$text <- gsub("(RT)((?:\\b\\w*@\\w+)+)","", tweets_slc$text)
tweets_slc$text <- gsub("^RT","", tweets_slc$text)
tweets_slc$text <- gsub("@\\w+","", tweets_slc$text)
tweets_slc$text <- gsub("[[:punct:]]","", tweets_slc$text)
tweets_slc$text <- gsub("[[:digit:]]+\\s","", tweets_slc$text)
tweets_slc$text <- gsub("http\\w+","", tweets_slc$text)
tweets_slc$text <- gsub("[ \t]{2,}"," ", tweets_slc$text)

tweets_slc$text <- iconv(tweets_slc$text, "UTF-8", "ASCII", sub="")

# Add id column.
tweets_slc$ID <- seq.int(nrow(tweets_slc))

head(tweets_slc, n=100)

```

### Word Frequency

```{r, eval=TRUE}

# Tokenize the text and see frequency of words
tweets_slc %>% 
  unnest_tokens(word, text)%>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE) 

# From above, we can see that words such as "president, trump" not pertaining to trade, so we remove them.
tweets_slc <- tweets_slc %>% mutate(text=tolower(text))
tweets_slc$text <- gsub("president?","", tweets_slc$text)
tweets_slc$text <- gsub("trump?","", tweets_slc$text)
head(tweets_slc,n=100)
  
# Retokenize the text and check to see if words being removed.
tweets_slc %>% 
  unnest_tokens(word, text)%>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE)

```

### Document Term Matrix

```{r, eval=TRUE}

# Select text and id column.
tweetscorpus <- tweets_slc %>% select(text)

# Create corpus for document term matrix.
tweetscorpus <- VCorpus(VectorSource(tweetscorpus))

# Convert the text to lowercase.
tweetscorpus <- tm_map(tweetscorpus, content_transformer(tolower))
tweetscorpus <- tm_map(tweetscorpus, PlainTextDocument)

# Remove all punctuation from the corpus.
tweetscorpus <- tm_map(tweetscorpus, removePunctuation)

# Remove all English stopwords from the corpus.
tweetscorpus <- tm_map(tweetscorpus, removeWords, stopwords("en"))
tweetscorpus <- tm_map(tweetscorpus, removeWords, stopwords("SMART"))

# Remove all number from the corpus.
tweetscorpus <- tm_map(tweetscorpus, removeNumbers)

# Strip extra white spaces in the corpus.
tweetscorpus <- tm_map(tweetscorpus, stripWhitespace)

# Stem the words in the corpus.
#tweetscorpus <- tm_map(tweetscorpus, stemDocument)

# Build document term matrix.
tweetsdtm<- DocumentTermMatrix(tweetscorpus)

# Remove sparse terms which don't appear very often. Limit the document term matrix to contain terms appearing in at least 5% of documents.
tweetsdtm <- removeSparseTerms(tweetsdtm, 0.95)

# Put the document in the format lda package required
tweetsdtm<- as.matrix(tweetsdtm)
head(tweetsdtm)

```

### Create Topic Modeling

```{r, eval=TRUE}
library(topicmodels)

tweetsLDA <- LDA(tweetsdtm, 5)

# Top 5 words per Topics
terms(tweetsLDA, 20)


```

## Sentiment Analysis

```{r, eval=TRUE}

# Turn tweets text into vector.
tweets.df <- as.vector(tweets_slc$text)

# Getting emotion score for each tweet.
tweets.emotion <- get_nrc_sentiment(tweets.df)
tweets.emotion <- cbind(tweets_slc, tweets.emotion) 
head(tweets.emotion)

# Getting sentiment score for each tweet.
tweets.score <- get_sentiment(tweets.df)
tweets.score <- cbind(tweets_slc,tweets.score )
head(tweets.score, n=100)

```

## Visualization 

### Plot Trump's tweets sentiment score on market price change.

````{r, eval=TRUE}
library(anytime)
library(dplyr)

# Update column name.
colnames(tweets.score)[2]<-"Date"

# Aggregate scores into single day.
tweets.score.sum <- tweets.score %>% 
  select(Date, tweets.score) %>% 
  group_by(Date) %>%
  summarise(scores=sum(tweets.score))

# Update date column into date format.
tweets.score.sum$Date <- anydate(tweets.score.sum$Date)
  
# Filter date to match with stocks dataframe.
tweets.score.flr <- tweets.score.sum %>% 
  filter(!is.na(Date)) %>%
  filter(between(Date, as.Date("2018-01-01"),as.Date("2019-11-20")))
  

# Bind stocks dataframe and scores dataframe.

stocks.df.new <-  stocks.df %>% select(Date, Pct_Change)


stocks.scores <- merge(stocks.df.new,tweets.score.flr, by='Date')

ggplot(stocks.scores, aes(Date)) + ggtitle("Stocks vs Scores") + ylab("%") +  geom_line(aes(y=Pct_Change, group=1, colour="% Change")) + geom_line(aes(y=scores, group=2, colour="Scores")) + theme(plot.title = element_text(hjust=0.5), axis.title.x=element_blank(), axis.text.x=element_text(angle=90,hjust=1), legend.position=c(0.5,0.9),legend.title=element_blank())


ggplot(stocks.scores,aes(x=Date)) +  geom_line(aes(y=scores, colour="Scores"))  + geom_line(aes(y=Pct_Change*10, colour="% Change")) + scale_y_continuous(sec.axis = sec_axis(~ ./100 , name = "% Change")) + scale_colour_manual(values=c("blue","red")) + labs(y="Scores", x="Date", colour="Parameter") + theme(legend.position=c(0.5,0.9))

```

Anil Version:
------------------------------------------------------------------------------------------------------------------------------------


```{r, eval=TRUE}

installed.packages('base64enc')
installed.packages('NLP')
installed.packages("textmineR")
installed.packages("anytime")
installed.packages("BiocManager")
installed.packages("topicmodels")
```


```{r, eval=TRUE}
# Load Requried Packages
library("base64enc")
library("SnowballC")
library("tm")
library("twitteR")
library("syuzhet")
library("dplyr")
library("tidyr")
library("tidytext")
library("textmineR")
library(purrr)
library(ggplot2)
library(readr)
library(textdata)
library(stringr)
library(lubridate)
library(hunspell)
library(lubridate)
library(anytime)
library(wordcloud)
library(BiocManager)
library(Rgraphviz)
library(topicmodels)
```

# Introduction

Donald Trump changed the communication platform of politics from burocratic aproaches of scheduled and managed political speeches to direct communication via Twitter. He started using twitter heavily on his 2016 persidential campaign and has not looked back since. His tweets has been analyzed by variety of researchers from frequency of "angry" tweets, his emotional state during the times of his tweets, the type of tweets he sends with specific mobile devices to his tweets impact on financial markets. In this study, the main business question we are trying solve is **"Can we leverage President Trump’s trade or interest rate related tweets and predict the market?"** We review the tweets between January 2018 to present, classify his tweets based on their topics and context related to trade wars, interest rate, employment in the US and conusmer spending , create a model and perform sentiment analysis. 

Overall the main goal of this study is to see the classified tweets of Donald Trump, discover possible relationship with the stock market and to see how the context of text used on his account impacts the stock market. In order to do this, we will identify and describe common topics and use of text that can change the market in the corpus of the tweets that is sent from the @realDonaldTrump twitter account. We can further compare the stock market data against these tweets to see if there is any correlation and if we can create a topic model and sentiment analysis that can predict the stock market.


# Data Collection

Based on the business problem in question, the content of the required data is Tweets and Stock Market Data. They are available via Twitter and Financial news platforms.

## Donald Trump's Tweets

Twitter's developer account provides many API procducts including tools to extract tweets and their metadata. We will use this API to extract the wtitter data in a structured format to further wrangling and analysis. In order to use the twitter API we created a twitter account and requested developer API access. Once we received an approval, we have been provided API key and Token access information. We will be using these keys, tokens to access the API and "twitterR" to extract Donald Trump's tweets. 

```{r, eval=TRUE}

consumer_key <- 'sPwbbZCtf8nfSMxhYTzqI8WHJ'
consumer_secret <- 'KfcOxgElcQ70fi3QNy8LkuDAN18dunXT147MoA8aBOLzpr3Vd3'
access_token <- '600477513-rdd3Fcywq1sfnh5S60egRQxXh0TlDqfrLzyZo4Vk'
access_secret <- 'SdDFCJUOoqAwt671VXeLaD781TdUYdeBSW2gyQMG4P5Zh'

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```


```{r}
trump_tweets <- userTimeline("realDonaldTrump", n=3200)
trump_tweets_df <- tbl_df(map_df(trump_tweets, as.data.frame))
head(trump_tweets_df)
```

Upon extracting tweeter data via Twitter API and converting to dataframe, we notice that there is a limitation on the number of tweets (3200) we can extract using twitter API. This is due to our account being "Free Developer Account" and in order us to increase the tweet account, we are required to upgrade our account. Since this might become problematic and can put a damper on our analysis and future model, we think it will be better to use a service called http://www.trumptwitterarchive.com/archive that archives all Donald Trump's tweets. 


```{r}

tweets_raw <- read.csv("trumptweets.csv", header = TRUE)
tweets <- tweets_raw %>%
  select(text, created_at, retweet_count, favorite_count) %>%
  rename(
    created=created_at,
    retweet=retweet_count,
    favorite=favorite_count
  )

tweets

```

Description of the variables in our data set is as follows;

* text: Content of the tweet.

* created: Date and Time the tweet is created.

* Retweet: The count of retweet of the tweet.

* Favorite: The count of favorited of the tweet.

## Stock Market Data 

There is an extensive amount of data available when it comes to Stock Market Data. Not for any partcular reason, we decided to use the available data in csv format from Yahoo Finance. 

```{r}

stock_raw <- read.csv("sandp.csv", header=TRUE)
head(stock_raw)

```

Description of the variables in our data set is as folows;

* Date: The Date of the Stock Market Ticket Value.

* Open: The Value of the ticket on open of the market.

* High: The highest value of the ticket during the trading date.

* Low: Lowest value of the ticket during the trading date.

* Close: The close value of the ticket during the trading date.

* Adj. Close: The value 

* Volume: The trading volume of the ticket during the trading date.

# Data Preperation

In this phase of the study, we will construct and clean the data set, including data cleaning and transforming. 

## Data Cleaning

We start with looking to see if there are any missing values and how we can handle them if there are any.

```{r}

na_count_tweets <- sapply(tweets, function(y) sum(length(which(is.na(y)))))
na_count_tweets <- data.frame(na_count_tweets)
na_count_stocks <- sapply(stock_raw, function(y) sum(length(which(is.na(y)))))
na_count_stocks <- data.frame(na_count_stocks)
na_count_tweets
na_count_stocks


```

We see that there are two missing values in tweets and there are no missing values in stock market dataset. Considering the amount of tweet data we have collected, we can ignore these two missing values.

```{r}
tweets <- na.omit(tweets)
str(tweets)
```
We have 10058 tweets, the retweet and favorite data types are numeric which is what we want. However created data type is factor and we would like that to be as Date format. We will format this data type to be Date.

```{r}

tweets$created <- mdy_hm(tweets$created)
tweets$created <- as.Date(tweets$created)
tweets
```

In our analysis, we are only looking at the 1 year of tweets so we will filter through the dataset. 

```{r}

tweets_year <- filter(tweets, created >= as.Date("2018-11-25") & created <= as.Date("2019-11-25"))
tweets_year

```

### Text Cleaning in Tweets

For text cleaning, we will follow the below steps;

* Create the Corpus

* Convert the text to lowercase

* Remove all the urls

* Remove non English characters and space

* Remove Stopwords

* Remove extra whitespace

In order to do this, we will use tm package.

```{r}
# in this one we need to add additional stop words to remove

tweet_corpus <- Corpus(VectorSource(tweets_year$text)) # building a corpus
removeURL <- function(x) gsub("http[^[:space:]]*", "", x) # to remove the urls
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)

tweet_corpus <- tweet_corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, c("amp", "realdonaldtrump", "rt", "will", stopwords("english"))) %>%
  tm_map(stripWhitespace)
  
tweet_corpus <- tm_map(tweet_corpus, content_transformer(removeURL)) # remove urls
tweet_corpus <- tm_map(tweet_corpus, content_transformer(removeNumPunct))
tweet_corpus_copy <- tweet_corpus # keep a copy for future stem completion 
tweet_corpus <- tm_map(tweet_corpus, stemDocument)
tweet_corpus[[1]]

```

We have created our Corpus and Stem Document, we can further inspect the tweets.

```{r}
tweet_corpus[1:5]
for (i in 1:20) {
    cat(paste("[[", i, "]] ", sep = ""))
    writeLines(as.character(tweet_corpus[[i]]))
}

```

We see there are some words and misspelling within the tweets that does not make sense. Such as "noth" for "nothing" or "ukrain" for "ukraine". We wont be replacing these not correct words with the correct ones since our data set is large enough to create our model. 

### Build Term Matrix Document

```{r}
tdm <- TermDocumentMatrix(tweet_corpus,
                          control = list(wordLengths = c(1, Inf)))
tdm

```


```{r}
# Frequency of Words and their association
idx <- which(dimnames(tdm)$Terms %in% c("trade", "interest", "war", "fed", "eu"))
as.matrix(tdm[idx, (1:50)])

```

# Data Exploration

```{r}
# frequent words
freq_terms <- findFreqTerms(tdm, lowfreq = 50)
freq_terms[0:10]

```

```{r}
term_freq <- rowSums(as.matrix(tdm))
term_freq <- subset(term_freq, term_freq >= 20) # at least 20 times
df <- data.frame(term = names(term_freq), freq = term_freq)
highest_df <- df[order(-df$freq), ]
top_20 <- top_n(highest_df, 20)
top_20

```


```{r}
theme_set(theme_classic())


ggplot(top_20, aes(x=term, y=freq))+
  geom_bar(stat="identity", width = 0.5, fill="tomato2")+
  xlab("Terms") + ylab("Count") + coord_flip() +
  theme(axis.text.x = element_text(angle=65, vjust=0.6, size=7))

```

Top two words that are used in the tweets are , "great" and "presid" . We need to update the stopwords I think. 

```{r}

wordcloud(tweet_corpus, max.words = 100, random.order = FALSE, rot.per = 0.15, min.freq = 5, colors = brewer.pal(8, "Dark2"))

```

```{r}

# which words are associated with 'trade'?
findAssocs(tdm, "trade", 0.2)

# which words are associated with 'interest'?
findAssocs(tdm, "interest", 0.2)

# which words are associated with 'jobs'?
findAssocs(tdm, "job", 0.2)

# which words are associated with 'jobs'?
findAssocs(tdm, "will", 0.2)


```

We can see trade is associated with eu and China, interest is associated with reserv, rate, cordial, inflat and borrow. I expected job or jobs and will to be associated with other words as well, however it didnt display any. 


```{r}
#visualizing the association (dont think we need it but if we want to show something fancy we can try)
plot(tdm, term = freq_terms, corThreshold = 0.35, weighting = T)


```


#  Topic Modelling

TRY using topic models - i think this is unsupervised tecnique. Need to research more. 
```{r}

dtm <- as.DocumentTermMatrix(tdm)
lda <- LDA(dtm, k, method = "Gibbs", control = list(nstart=nstart))
term <- terms(lda, 10)
term <- apply(term, MARGIN = 2, paste, collapse = ", ")


```

TRYING CLUSTERING WORDS 

```{r}
# tdm is 100% sparse - remove sparse terms

sparse_tdm <- removeSparseTerms(tdm, sparse = 0.95) #95% - i will add more write up on this
sparse_matrix <- as.matrix(sparse_tdm)

```

```{r}
#cluster terms

clust_matrix <- dist(scale(sparse_matrix))
fit <- hclust(clust_matrix, method = "ward.D")
plot(fit)

```

```{r}

# cut tree into 10 clusters
rect.hclust(fit, k=5)
(groups <- cutree(fit, k=10))

```

For Topic Modelling we can try to use topicsmodel package which is LDA and Sentiment Analysis there are packages such as sentiment140 i think tidytext packahe available. I tried to use the topicsmodel but was not successfull. I think we can also use k-means algorithm for clustering. I started using that and was getting somewhere but need to revisit later.


Reference http://www.rdatamining.com/ RDataMining - 


Joe Version:
-----------------------------------------------------------------------------------------------------------------------------------

***Delete this later as it is a guide for Python need to get methods for R ***

https://towardsdatascience.com/trump-tweets-and-trade-96ac157ef082

Can a S&P 500 trading strategy take advantage of President Trump’s trade-related tweets and predict short-term moves in the market? I turned to topic modeling and sentiment analysis to find out. The general project design and workflow is as follows:
1) Data Collection and Preprocessing: raw tweets were collected and preprocessed to pass on tokenized and lemmatized words to the topic model.
2) Topic Modeling with LDA: topic modeling was utilized to assign each tweet a topic. Only tweets related to the trade topic were passed along for sentiment analysis.
3) Sentiment Analysis with VADER: after identifying tweets related to trade, each tweet was assigned a sentiment score. Sentiment scores were passed along as the core input to the trading strategy.
4) Backtesting Trading Strategy on S&P 500 Data: once sentiment scores were assigned, a custom trading strategy was designed to trade on significant sentiment scores.



```{r}
# Load stock data files and Tweets from Trump 
# Step 1 of the process flow described above. 
snp_raw <- read.csv("sandp.csv", header = TRUE)
typeof(snp_raw)
snp_raw[2,]
tweets_raw <- read.csv("trumptweets.csv", header = TRUE)
typeof(tweets_raw)
tweets_raw[19:20,]
stop_words <- read.csv("stopwords.csv", header=TRUE)
```

Need to add preprocessing steps:
The following preprocessing steps were performed with the Gensim and nltk python libraries (NEED TO FIGURE out how to DO THIS IN R):
- Tweets that contained less than 20 characters (375 tweets in total) were treated as noise and subsequently dropped.
- Tweets were tokenized; sentences were split into individual words, words were made all lowercase, and punctuation was removed.
- Default stop words from nltk were used, as well as custom stop words. The process for identifying custom stop words was iterative and included words such as “fake, news, James Comey, Puerto Rico, etc.”, words that dominated tweet topics not pertaining to trade.
- Words were lemmatized to reduce each word to its base form.
- Bigrams were made for topic modeling later on. Bigrams were chosen to capture context surrounding individual phrases.
- Finally, to make the data gensim friendly, a bag of words dictionary detailing how many times a word appears in all tweets was created. Gensim also requires a corpus — a dictionary was created for each tweet containing how many words appear in the tweet and how many times each word appears in the tweet.


Pre-processing
As we observe from the text, there are many tweets which consist of irrelevant information: such as RT, the twitter handle, punctuation, stopwords (and, or the, etc) and numbers. These will add unnecessary noise to our dataset which we need to remove during the pre-processing stage.

```{r}
#later take off the limit head 
tweets_sel <- tweets_raw %>% select(id_str, text) %>% head(20)
typeof(tweets_sel)              
tweets_sel

# pre-processing
tweets_sel$text <- sub("RT.*:", "", tweets_sel$text)
tweets_sel$text <- sub("@.* ", "", tweets_sel$text)
tweets_sel
text_cleaning_tokens <- tweets_sel %>% 
  tidytext::unnest_tokens(word, text)
text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)
# adding csvs for checking can be removed later - JR
write.csv(text_cleaning_tokens,"~/Documents/CUNY/Data607/fproject/Twitter/wordtoken.csv", row.names = FALSE)
# stop_words needs to be further added to also can remove the below two CSV files. 
# using CSV files to check if stop_words is removing from the word tokens
text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)
write.csv(text_cleaning_tokens,"~/Documents/CUNY/Data607/fproject/Twitter/wordtokenstopwords.csv", row.names = FALSE)
tokens <- text_cleaning_tokens %>% filter(!(word==""))
tokens <- tokens %>% mutate(ind = row_number())
tokens <- tokens %>% group_by(id_str) %>% mutate(ind = row_number()) %>%
  tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] <- ""
tokens <- tidyr::unite(tokens, text,-id_str,sep =" " )
tokens$text <- trimws(tokens$text)

```
#Topic Modeling with LDA
https://towardsdatascience.com/beginners-guide-to-lda-topic-modelling-with-r-e57a5a8e7a25

3- Model Building - create a DTM(document term matrix)
 In our case, because it’s Twitter sentiment, we will go with a window size of 1–2 words, and let the algorithm decide for us, which are the more important phrases to concatenate together. We will also explore the term frequency matrix, which shows the number of times the word/phrase is occurring in the entire corpus of text. If the term is < 2 times, we discard them, as it does not add any value to the algorithm, and it will help to reduce computation time as well.
```{r}
#create DTM
dtm <- CreateDtm(tokens$text, 
                 doc_names = tokens$id_str, 
                 ngram_window = c(1, 2))
#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the
# documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm
dtm
```

