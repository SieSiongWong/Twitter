---
title: "Final Project"
author: "Sie Siong Wong - Joe Rovalino - Anil Akyildirim"
date: "11/8/2019"
output: html_document
---

```{r, eval=TRUE}

installed.packages('base64enc')
installed.packages('NLP')

```

```{r, eval=TRUE}
# Load Requried Packages
library("base64enc")
library("SnowballC")
library("tm")
library("twitteR")
library("syuzhet")
library("dplyr")
library("tidyr")
```

```{r, eval=TRUE}

consumer_key <- 'sPwbbZCtf8nfSMxhYTzqI8WHJ'
consumer_secret <- 'KfcOxgElcQ70fi3QNy8LkuDAN18dunXT147MoA8aBOLzpr3Vd3'
access_token <- '600477513-rdd3Fcywq1sfnh5S60egRQxXh0TlDqfrLzyZo4Vk'
access_secret <- 'SdDFCJUOoqAwt671VXeLaD781TdUYdeBSW2gyQMG4P5Zh'

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```

```{r}
tweets <- userTimeline("realDonaldTrump",n=2)
tweets
```
***Delete this later as it is a guide for Python need to get methods for R ***

https://towardsdatascience.com/trump-tweets-and-trade-96ac157ef082

Can a S&P 500 trading strategy take advantage of President Trump’s trade-related tweets and predict short-term moves in the market? I turned to topic modeling and sentiment analysis to find out. The general project design and workflow is as follows:
1) Data Collection and Preprocessing: raw tweets were collected and preprocessed to pass on tokenized and lemmatized words to the topic model.
2) Topic Modeling with LDA: topic modeling was utilized to assign each tweet a topic. Only tweets related to the trade topic were passed along for sentiment analysis.
3) Sentiment Analysis with VADER: after identifying tweets related to trade, each tweet was assigned a sentiment score. Sentiment scores were passed along as the core input to the trading strategy.
4) Backtesting Trading Strategy on S&P 500 Data: once sentiment scores were assigned, a custom trading strategy was designed to trade on significant sentiment scores.



```{r}
# Load stock data files and Tweets from Trump 
# Step 1 of the process flow described above. 
snp_raw <- read.csv("sandp.csv", header = TRUE)
typeof(snp_raw)
snp_raw[2,]
tweets_raw <- read.csv("trumptweets.csv", header = TRUE)
typeof(tweets_raw)
tweets_raw[19:20,]
stop_words <- read.csv("stopwords.csv", header=TRUE)
```

Need to add preprocessing steps:
The following preprocessing steps were performed with the Gensim and nltk python libraries (NEED TO FIGURE out how to DO THIS IN R):
- Tweets that contained less than 20 characters (375 tweets in total) were treated as noise and subsequently dropped.
- Tweets were tokenized; sentences were split into individual words, words were made all lowercase, and punctuation was removed.
- Default stop words from nltk were used, as well as custom stop words. The process for identifying custom stop words was iterative and included words such as “fake, news, James Comey, Puerto Rico, etc.”, words that dominated tweet topics not pertaining to trade.
- Words were lemmatized to reduce each word to its base form.
- Bigrams were made for topic modeling later on. Bigrams were chosen to capture context surrounding individual phrases.
- Finally, to make the data gensim friendly, a bag of words dictionary detailing how many times a word appears in all tweets was created. Gensim also requires a corpus — a dictionary was created for each tweet containing how many words appear in the tweet and how many times each word appears in the tweet.

```{r}
#later take off the limit head 
tweets_sel <- tweets_raw %>% select(id_str, text) %>% head(20)
typeof(tweets_sel)              
tweets_sel

# pre-processing
tweets_sel$text <- sub("RT.*:", "", tweets_sel$text)
tweets_sel$text <- sub("@.* ", "", tweets_sel$text)
tweets_sel
text_cleaning_tokens <- tweets_sel %>% 
  tidytext::unnest_tokens(word, text)
text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)
# adding csvs for checking can be removed later - JR
write.csv(text_cleaning_tokens,"~/Documents/CUNY/Data607/fproject/Twitter/wordtoken.csv", row.names = FALSE)
stop_words
text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)
write.csv(text_cleaning_tokens,"~/Documents/CUNY/Data607/fproject/Twitter/wordtokenstopwords.csv", row.names = FALSE)
tokens <- text_cleaning_tokens %>% filter(!(word==""))
tokens <- tokens %>% mutate(ind = row_number())
tokens <- tokens %>% group_by(id_str) %>% mutate(ind = row_number()) %>%
  tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] <- ""
tokens <- tidyr::unite(tokens, text,-id_str,sep =" " )
tokens$text <- trimws(tokens$text)

```
#Topic Modeling with LDA
https://towardsdatascience.com/beginners-guide-to-lda-topic-modelling-with-r-e57a5a8e7a25
```{r}

```

